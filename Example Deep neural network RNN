#Sample data from http://rpubs.com/juanhklopper/deep_neural_network_example
#Youtube - https://www.youtube.com/watch?v=CHJzoFArI8c

library(readr)
library(keras)
library(DT)
data.set <- read_csv("SimulatedBinaryClassificationDataset.csv",
                     col_names = TRUE)
datatable(data.set[sample(nrow(data.set),
                          replace = FALSE,
                          size = 0.01 * nrow(data.set)), ])
summary(data.set)

# Cast dataframe as a matrix
data.set <- as.matrix(data.set)

# Remove column names
dimnames(data.set) = NULL

# Split for train and test data (Kept at 0.9 and 0.1 together for now, change to 0.7 and 0.3)
set.seed(123)
indx <- sample(2,
               nrow(data.set),
               replace = TRUE,
               prob = c(0.9, 0.1)) # Makes index with values 1 and 2


# Select only the feature variables
# Take rows with index = 1
x_train <- data.set[indx == 1, 1:10]
x_test <- data.set[indx == 2, 1:10]

#Target variable split - 
y_test_actual <- data.set[indx == 2, 11]

# Using similar indices to correspond to the training and test set
y_train <- to_categorical(data.set[indx == 1, 11])
y_test <- to_categorical(data.set[indx == 2, 11])

#Show first five target variables of the test set
cbind(y_test_actual[1:10],
      y_test[1:10, ])

# Creating the model
model <- keras_model_sequential()

model %>% 
  layer_dense(name = "DeepLayer1",
              units = 10,
              activation = "relu",
              input_shape = c(10)) %>% 
  layer_dense(name = "DeepLayer2",
              units = 10,
              activation = "relu") %>% 
  layer_dense(name = "OutputLayer",
              units = 2,
              activation = "softmax")

summary(model)
#As designed, this shows There are three columns in the summary,
#the first giving the layer name (as optionally specified when the network was created) 
#and its type. All of the layers are densely connected layers in this example. 
#The Output Shape column specifies the output shape (after tensor multiplication, bias addition, 
#and activation, i.e. forward propagation). The Param # column indicates the number of parameters 
#(weights and biases) that the specific layer must learn. For layer one (feature variables), since 
#there was 10 input nodes connected to 10 nodes in the first hidden layer, that results in 10Ã—10=100 
#parameters plus the column vector of bias values, of which there are also 10, resulting in 110 
#parameters. The next two layers follow a similar explanation.
#showing all 242 parameters (weight and bias) values that are to be optimized (minimizing the cost 
#function), through backpropagation and gradient descent.


#Compiling the model
model %>% compile(loss = "categorical_crossentropy",
                  optimizer = "adam",
                  metrics = c("accuracy"))

#Fitting the data
history <- model %>% 
  fit(x_train,
      y_train,
      epoch = 10,
      batch_size = 256,
      validation_split = 0.1,
      verbose = 2)

#Gives output of the plot
plot(history)

# Model Evaluation
model %>% 
  evaluate(x_test,
           y_test)

pred <- model %>% 
  predict_classes(x_test)

table(Predicted = pred,
      Actual = y_test_actual)

prob <- model %>% 
  predict_proba(x_test)

#This prints the first 5 probabilities
1 - prob[1:5]

cbind(1 - prob[1:10],
      pred[1:10],
      y_test_actual[1:10])

